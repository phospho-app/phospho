{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phospho quickstart\n",
    "\n",
    "In this quickstart, we will use the `lab` from the `phospho` package to figure out how many messages in a dataset are questions.\n",
    "\n",
    "1. First, we will detect events on a subset of the dataset using a pipeline powered by OpenAI GPT 3.5\n",
    "\n",
    "2. Then, we will scale analytics with the `lab` optimizer. We will compare the event detection pipeline using MistralAI and a local Ollama model, and pick the best one in term of performance, speed and price.\n",
    "\n",
    "3. Finally, we will use the `lab` to run the best model on the full dataset and visualize the results.\n",
    "\n",
    "This way, we will be able to run semantic analytics at scale on a dataset using LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q python-dotenv \"phospho[lab]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check env variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from phospho import config\n",
    "\n",
    "assert (\n",
    "    config.OPENAI_API_KEY is not None\n",
    "), \"You need to set the OPENAI_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Event detection pipeline\n",
    "\n",
    "In phospho, there are two important concepts:\n",
    "\n",
    "- A workload, which is a set of jobs. Those jobs are run asynchronously and in parallel.\n",
    "- A job, which is a python function that returns a JobResult. Jobs are parametrized with a JobConfig.\n",
    "\n",
    "In this example, the Job is to detect an event (\"Event Detection\") using LLM self-reflection (we asked another LLM whether the event occured or not). We will try to detect the event: \"The user asks a question to the assistant\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phospho import lab\n",
    "\n",
    "# Create a workload in our lab\n",
    "workload = lab.Workload()\n",
    "\n",
    "# Add our job to the workload\n",
    "workload.add_job(\n",
    "    lab.Job(\n",
    "        name=\"event_detection\",\n",
    "        id=\"question_answering\",\n",
    "        config=lab.EventConfig(\n",
    "            event_name=\"Question Answering\",\n",
    "            event_description=\"User asks a question to the assistant\",\n",
    "            model=\"openai:gpt-3.5-turbo\",\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can run the pipeline on a Message.\n",
    "\n",
    "We want to detect whether the user asks a question. Sometimes, it's easy: there is a question mark. But sometimes, it's not: you understand that it is a question only through context and semantics. That's why you need an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In message 1, the Event question answering was detected: True\n",
      "In message 2, the Event question answering was detected: False\n",
      "In message 3, the Event question answering was detected: True\n"
     ]
    }
   ],
   "source": [
    "await workload.async_run(\n",
    "    messages=[\n",
    "        # This message is a question, very simple to detect.\n",
    "        lab.Message(\n",
    "            id=\"message_1\",\n",
    "            content=\"What is the capital of France?\",\n",
    "        ),\n",
    "        # This message is not a question, so it should not be detected.\n",
    "        lab.Message(\n",
    "            id=\"message_2\",\n",
    "            content=\"I don't like croissants.\",\n",
    "        ),\n",
    "        # This message is also a question, but it lacks a question mark. You need semantics to detect it.\n",
    "        lab.Message(\n",
    "            id=\"message_3\",\n",
    "            content=\"I wonder what's the capital of France...\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    print(\n",
    "        f\"In message {i}, the Event question answering was detected: {workload.results['message_'+str(i)]['question_answering'].value}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analytics\n",
    "\n",
    "Now, let's assume we want to find user questions in a large dataset. How would we do it?\n",
    "\n",
    "Let's load a dataset of messages from huggingface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daily_dialog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has more than 10 000 samples. That's a lot and running analytics on it can quickly become pricy. So let's just select a subsample of it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialog', 'act', 'emotion'],\n",
       "    num_rows: 11118\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say , Jim , how about going for a few beers after dinner ? \n"
     ]
    }
   ],
   "source": [
    "# Generate a sub dataset with 30 messages\n",
    "sub_dataset = dataset[\"train\"].select(range(30))\n",
    "\n",
    "# Let's print one of the messages\n",
    "print(sub_dataset[0][\"dialog\"][0])\n",
    "\n",
    "# Build the message list for our lab\n",
    "messages = []\n",
    "for row in sub_dataset:\n",
    "    text = row[\"dialog\"][0]\n",
    "    messages.append(lab.Message(content=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the analytics pipeline on the subset.\n",
    "\n",
    "The workload run is **asynchronous** and **parallelized**, which means this will go much faster than just writing a for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the lab on it\n",
    "# The job will be run with the default model (openai:gpt-3.5-turbo)\n",
    "workload_results = await workload.async_run(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 23b0aa7c1f134208b280acbe6a244ede was a question: False\n",
      "Message 344a2c917fc24e71b7ae23c624d59e14 was a question: True\n",
      "Message 2cce5553503d4c08a0711659b9922523 was a question: True\n",
      "Message 6a9d429d811c4a2881170a2baee1b67a was a question: False\n",
      "Message e102b2e413054470a4700a2a4f8f5b06 was a question: False\n",
      "Message 8d8e544d122746ac995b6cb340581e84 was a question: True\n",
      "Message a94f4697fb884d66baec0e4f4b59a2f9 was a question: False\n",
      "Message 4601a36a85cb4fd29f9b7d90dcb1cf6e was a question: False\n",
      "Message 4c5f8dc4991147f1ac23c6b098dbaafa was a question: False\n",
      "Message 5f3ff3adb3ea40f0aa81da9156dffc50 was a question: True\n",
      "Message f9952fa16d594d50bf35bd52474b5122 was a question: True\n",
      "Message 82d98dc65d084396aaf07bf8c0d8571e was a question: False\n",
      "Message 7a18c5a2e529459fa53140bef2fa4de3 was a question: False\n",
      "Message 4581cfbdf1354182a82e82c879ca672a was a question: True\n",
      "Message 37f1f7ebda884a4a897a8129e56be57d was a question: False\n",
      "Message cb4672258f5c4f53ae51bddf134ff49b was a question: False\n",
      "Message 5066d2241d094bc1816cd3ef2b4ed94e was a question: True\n",
      "Message d417b20c40b74f07b7f58a9d4a7e9182 was a question: True\n",
      "Message 0c984ec7781d4e44948bff9d78c54393 was a question: True\n",
      "Message a80759c7ea5644e98ddf8c44f999fdcb was a question: True\n",
      "Message b117e626244045ae939c2a198b5fe46f was a question: False\n",
      "Message 2a4dc23e6ae84f65bd70413541ceaf4b was a question: True\n",
      "Message 9b51a80dcbdc41f09bc52930723f9186 was a question: True\n",
      "Message 947f7d4f3ec84aad99254be8fc28535b was a question: False\n",
      "Message a1a04fd0b19c4b77b8e5a3193f7d7b5e was a question: True\n",
      "Message 54d01923c4784e6e91c5e1559246f322 was a question: False\n",
      "Message 21b8d7346c224fada7eb8e7d3b7aa52c was a question: True\n",
      "Message ee98cd3706b44e3d9896ee357bf31211 was a question: False\n",
      "Message 6f76655f5648475580c9060e329c5359 was a question: False\n",
      "Message 0526fad4b49143218cc1ad5465334303 was a question: True\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for message_id, jobs in workload_results.items():\n",
    "    print(f\"Message {message_id} was a question: {jobs['question_answering'].value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the pipeline\n",
    "\n",
    "Running semantic analytics with an LLM is great. But it's expensive and slow.\n",
    "\n",
    "You likely want to try other model providers, such as Mistral, or even small local models. But how do they compare?\n",
    "\n",
    "Let's run the pipeline on these models, and then figure out which one matches the reference, GPT-4.\n",
    "\n",
    "For the purpose of this demo, we consider a considertion good enough if it matches gpt-4 on at least 80% of the dataset. Good old Paretto.\n",
    "\n",
    "### Installation and setup\n",
    "\n",
    "You will need:\n",
    "\n",
    "- a Mistral AI API key (find yours [here](https://console.mistral.ai/api-keys/))\n",
    "- Ollama running on your local machine, with the Mistral 7B model installed. You can find the installation instructions for Ollama [here](https://ollama.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phospho import config\n",
    "\n",
    "# Check the environment variable\n",
    "assert (\n",
    "    config.MISTRAL_API_KEY is not None\n",
    "), \"You need to set the MISTRAL_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Camembert.\"\n"
     ]
    }
   ],
   "source": [
    "from phospho.lab.language_models import get_sync_client\n",
    "\n",
    "# Create a client\n",
    "ollama = get_sync_client(\"ollama\")\n",
    "\n",
    "try:\n",
    "    # Let's check we can reach your local Ollama API\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the best French cheese? Keep your answer short.\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\n",
    "        \"You need to have a local Ollama server running to continue and the mistral model downloaded. \\nRemove references to Ollama otherwise.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the results with the alternative configurations\n",
    "\n",
    "To run the jobs on multiple models at the same time, we will simply set up our job with a different configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "workload = lab.Workload()\n",
    "\n",
    "\n",
    "# Setup the configs for our job\n",
    "class EventConfig(lab.JobConfig):\n",
    "    event_name: str = \"Question Answering\"\n",
    "    event_description: str = \"User asks a question to the assistant\"\n",
    "    # Model are ordered from the least desired to the most desired\n",
    "    # The default model is set to be the \"reference\"\n",
    "    model: Literal[\n",
    "        \"openai:gpt-4\",\n",
    "        \"mistral:mistral-large-latest\",\n",
    "        \"mistral:mistral-small-latest\",\n",
    "        \"ollama:mistral\",\n",
    "    ] = \"openai:gpt-4\"\n",
    "\n",
    "\n",
    "# Add our job to the workload\n",
    "workload.add_job(\n",
    "    lab.Job(\n",
    "        name=\"event_detection\",\n",
    "        id=\"question_answering\",\n",
    "        config=EventConfig(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the workload in the same way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_results = await workload.async_run(messages=messages, executor_type=\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openai:gpt-4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the default model is currently set to \"openai:gpt-4\"\n",
    "workload.jobs[0].config.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's also run the pipeline on alternative models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute alternative results with the Mistral API and Ollama\n",
    "await workload.async_run_on_alternative_configurations(\n",
    "    messages=messages, executor_type=\"parallel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask the workload to figure out which model is better.\n",
    "\n",
    "Note that this can actually work with any set of parameters, not just models. It's a flexible way to perform a grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies: [0.8333333333333334, 0.8, 0.5666666666666667]\n"
     ]
    }
   ],
   "source": [
    "workload.optimize_jobs(accuracy_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what model was picked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral:mistral-small-latest'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the new model_id (if it has changed)\n",
    "workload.jobs[0].config.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We can run our pipeline with roughly the same accuracy using a smaller model. That's a lot of time, compute and money saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our workload on the full dataset, with optimized parameters\n",
    "\n",
    "Now that we have benchmarked different models for our Event detection pipeline, let's run the optimal configuration on a larger chunk of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = dataset[\"train\"].select(\n",
    "    range(200)\n",
    ")  # Here you can just leave it as dataset[\"train\"] if you want to use the whole dataset\n",
    "\n",
    "# Build the message list for our lab\n",
    "messages = []\n",
    "for row in sub_dataset:\n",
    "    text = row[\"dialog\"][0]\n",
    "    messages.append(lab.Message(content=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run the evals on a lot of messages in parallel. However, if we just bombarded the model provider API, we'd have a **rate limit error.**\n",
    "\n",
    "Indeed, each model provider has their own limitation. For Mistral, it's 5 requests per second max per default.\n",
    "\n",
    "In order to avoid that, **we set a throttle of maximum 5 requests per second** using the parameter `max_parallelism`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The job will be runned with the best model (mistral:mistral-small-latest in our case)\n",
    "workload_results = await workload.async_run(\n",
    "    messages=messages,\n",
    "    executor_type=\"parallel\",\n",
    "    max_parallelism=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the results\n",
    "\n",
    "Now, we were trying to see which share of the dataset is actually a question. Let's get the results as a dataframe and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_answering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e4c6e973db4a4eebacaa2befab6a7d50</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228b270d2ae4689bf78902e3481eb99</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7a59050473174537866186f32dcbdfd8</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bb287be6b48a4425b08e402c89bc6944</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f561e8350af148949c21e8ea2dec7764</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278bcbca28f34614af78be0c98219300</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54ce64aa30f847c48dab8d36932f9cf2</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd37341a5410418393722b54f671a5a9</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>083b8d38700f4b92b3b177ee7d4663ac</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39d5fc640bc34555b820f519fe738c47</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  question_answering\n",
       "e4c6e973db4a4eebacaa2befab6a7d50               False\n",
       "6228b270d2ae4689bf78902e3481eb99               False\n",
       "7a59050473174537866186f32dcbdfd8                True\n",
       "bb287be6b48a4425b08e402c89bc6944               False\n",
       "f561e8350af148949c21e8ea2dec7764               False\n",
       "...                                              ...\n",
       "278bcbca28f34614af78be0c98219300                True\n",
       "54ce64aa30f847c48dab8d36932f9cf2                True\n",
       "dd37341a5410418393722b54f671a5a9                True\n",
       "083b8d38700f4b92b3b177ee7d4663ac                True\n",
       "39d5fc640bc34555b820f519fe738c47                True\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using .results_df() we can get a pandas dataframe with the results\n",
    "workload.results_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many questions in this dataset...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_answering\n",
       "False    108\n",
       "True      92\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workload.results_df().groupby(\"question_answering\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a nice plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/nicolasoulianov/anaconda3/envs/phospho-env/lib/python3.11/site-packages/typing_extensions-4.8.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='question_answering'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workload.results_df().groupby(\"question_answering\").size().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "You can use the `lab` to run other tasks, such as:\n",
    "\n",
    "- Named Entity Recognition\n",
    "- Sentiment Analysis\n",
    "- Evaluations\n",
    "- And more!\n",
    "\n",
    "You can also play around with differnet models, different hyperparameters, and different datasets.\n",
    "\n",
    "You want to have such analysis on your own LLM app, in real time? Check out the cloud hosted version of phospho, available on [phospho.ai](https://phospho.ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
